# プロジェクトコンセプト

## 背景

前職（医療事務）で、毎日100〜200件の患者データをCSV/PDFから手動でSQL Serverに入力していました。
テンプレートが提供者ごとにバラバラで、typoや金額ミスも多く、正直かなり非効率な状態でした。

この経験から「ETLで自動化すればよかったのに」と思い、実際にPythonで作ってみたのがこのプロジェクトです。

## 何を作ったか

CSVで届く医療費データを、自動でクレンジングしてDBに入れ、APIで提供する仕組みです。

```
CSV → ETL（クレンジング） → PostgreSQL → REST API
```

実務で実際に起きていた問題に対応するクレンジング処理を入れています：

- **空白の混入** → CSVエクスポート時にスペースが入ることがあった → `strip()`
- **患者IDなし** → IDが空だと追跡できない → `dropna`
- **不正な日付** → 手入力で `invalid-date` みたいなのが混ざる → `pd.to_datetime(errors="coerce")`
- **マイナス金額** → 入力ミスで請求エラーになる → `billed_amount > 0`
- **重複登録** → 同じレコードが二重に入る → `drop_duplicates()`

## 技術的な判断

### FastAPIにした理由

FlaskでもDjangoでもよかったけど、FastAPIは型ヒントを書くだけでSwagger UIのドキュメントが自動生成されるのが便利だった。API専用なので余計な機能がなくて軽い。

### ORMを使った理由

SQLを直接書いてもいいけど、SQLAlchemy経由にしておけばDB変更時（PostgreSQL → MySQL等）にコード側の修正が少なくて済む。テーブル定義がコードに残るのもメリット。

### マルチステージビルド

Dockerfileをシングルステージで書くとpipやgccがイメージに残って重くなる。ビルド用と本番用でステージを分けて、本番イメージにはライブラリだけコピーするようにした。

### CI/CD

GitHub Actionsで、pushやPRのたびにPostgreSQLのサービスコンテナを立ててpytestを実行している。ローカルで「動いたつもり」になるのを防ぐため。

## 実務との対応

| 実際の現場 | このプロジェクト |
|-----------|---------------|
| CSV/PDFから手動入力 | CSV → ETL自動処理 |
| SQL Server | PostgreSQL |
| 100-200件/日 | サンプルデータ15件 |
| チェック体制なし | pytestで自動テスト |
| ローカル環境のみ | Docker + CI/CD |

## 今後やりたいこと

- JWT認証をつけてAPI保護
- Redisでキャッシュ
- ログの構造化（JSON形式）
- 推論API（医療費の異常検知）

## 面談用メモ

**Q: 直近のプロジェクトについて教えてください**

前職で医療費データを毎日手動入力していた経験から、ETLパイプラインで自動化するプロジェクトを作りました。CSVからデータを読み込んで、不正データ（空白、無効な日付、マイナス金額、重複）をクレンジングしてDBに投入し、APIで提供する構成です。FastAPIで作って、Docker + docker-composeで環境構築、GitHub Actionsでテスト自動化しています。

**Q: なぜFastAPIを選んだ？**

型ヒントを書くだけでSwagger UIが自動生成されるのが便利で、APIに特化しているので余計な機能がなくて扱いやすかったです。

**Q: Dockerの経験は？**

マルチステージビルドでイメージサイズを最適化しています。docker-composeでAPI + PostgreSQLを管理して、DBのヘルスチェックをかけてから APIを起動するようにしています。

**Q: テストは？**

pytestでETL処理のユニットテスト（金額や日付の検証）とAPIの統合テスト（TestClient）を書いています。CIで毎回回しています。
