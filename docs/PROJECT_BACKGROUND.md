# 🏥 プロジェクト背景 - 面談で話すストーリー

## プロジェクトの背景（Before）

- 医療現場で様々なテンプレートで記入された **患者データ・医療費データ** を扱っていた
- これらを **手動で SQL Server に入力** していた
- 毎日 **100〜200人** の新規患者データを処理
- **課題**: 手動入力によるミス（typo、金額ミス）と処理遅延

## プロジェクトの目的（After）

**ETLパイプライン** をPythonで構築し、データ処理を自動化する：

| フェーズ | 内容 |
|---------|------|
| **Extract（抽出）** | PDF・CSV など統一されていない形式から患者データを読み取る |
| **Transform（加工）** | 重要な項目を抽出、フォーマット統一、バリデーション |
| **Load（格納）** | クレンジング済みデータを DB に自動投入 |

## 技術的なチャレンジ

- データ提供者ごとにテンプレートが **バラバラ**
- テンプレに少しでもズレがあると従来のルールベースでは処理できない
- → **機械学習** を使い、テンプレからズレたデータも自動で認識・ETL処理に通せるようにした

## このミニプロジェクトとの対応

| 実際の現場 | このミニプロジェクト |
|-----------|-------------------|
| 手動入力 → 自動化 | CSV → ETL → API |
| SQL Server | FastAPI + Python |
| 100-200人/日 | サンプルデータ |
| ML でテンプレ認識 | Step 9 で推論API |
| 手動確認 → 自動テスト | Step 2 で pytest |

## 面談回答テンプレート

> **Q: 直近のプロジェクトについて教えてください**
>
> A: 「医療現場で、毎日100〜200件の患者データをPDF・CSVから手動で入力する作業がありました。
> ミスや遅延が課題だったので、PythonでETLパイプラインを構築して自動化しました。
> Extract で様々な形式のファイルからデータを抽出、Transform で正規化とバリデーション、
> Load でDBに投入する流れです。
> テンプレートが統一されていないデータにも対応するため、機械学習を活用しました。
> APIはFastAPIで作り、テスト自動化にはpytest、CI/CDにはGitHub Actionsを使いました。」
